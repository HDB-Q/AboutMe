{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: Short Term Rental\n",
        "filters:\n",
        "  - lightbox\n",
        "lightbox: auto\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### Tracking Airbnb in Toronto with {python}\n",
        "\n",
        "The City of Toronto maintains several [open datasets](https://www.toronto.ca/city-government/data-research-maps/open-data/) related to city operations - updated at a range of frequencies.  This project is focused on the Short Term Rental data (updated daily). This data is useful in tracking active Airbnb and similar short term accomodations in Toronto. However, the data has some limitations - mainly that only active listings are available, not a history. \n",
        "\n",
        "Per the given metadata, *A short-term rental is all or part of a dwelling unit rented out for less than 28 consecutive days in exchange for payment. Short-term rentals include bed and breakfasts, but exclude hotels, motels, student residences owned or operated by publicly funded or non-profit educational institutions.* See the City's [page](https://www.toronto.ca/community-people/housing-shelter/short-term-rentals/) for more information.\n",
        "\n",
        "\n",
        "In this article, we walk through a practical example of managing and updating a database of short-term rental listings using {python} and SQLite. The goal is to maintain an up-to-date record of rental properties by regularly incorporating new data and updating existing records. Here’s a step-by-step guide on how this can be achieved.\n",
        "\n",
        "#### Setting Up the Environment\n",
        "1. **Package Imports:**\n",
        "   To begin, we import necessary {python} packages including:\n",
        "   - `pandas` for data manipulation, \n",
        "   - `sqlite3` for database interactions, \n",
        "   - `hashlib` for hashing, and \n",
        "   - a custom script (`ShortTermRental`) to fetch new rental data.\n",
        "\n",
        "2. **Hashing Function:**\n",
        "   A hash function is used to create a unique identifier for each rental record based on specific columns such as `operator_registration_number`, `address`, `unit`, `postal_code`, `property_type`, `ward_number`, and `ward_name`. This identifier helps in quickly comparing new records with existing ones to identify updates or new entries.\n",
        "\n",
        "3. **Fetching from Toronto's Open Data**\n",
        "#### Setting Up the Environment\n",
        "\n",
        "To get started, we need to set up our {python} environment by importing the necessary packages. In this case, we'll use `requests` to make API calls, `pandas` for data manipulation, and `datetime` to handle date and time information.\n"
      ],
      "id": "d3e59056"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "from datetime import datetime, date"
      ],
      "id": "9a5a1e13",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Fetching Rental Data\n",
        "\n",
        "The core of our data collection process involves hitting an API to retrieve short-term rental data. The following steps outline how to achieve this:\n",
        "\n",
        "1. **Define the API Endpoint:**\n",
        "   We first define the base URL for the API provided by the City of Toronto’s open data portal. The endpoint for accessing data packages is constructed as follows:\n"
      ],
      "id": "3438fc0e"
    },
    {
      "cell_type": "code",
      "metadata": {
        "md-indent": "   "
      },
      "source": [
        "base_url = \"https://ckan0.cf.opendata.inter.prod-toronto.ca\"\n",
        "url = base_url + \"/api/3/action/package_show\"\n",
        "params = { \"id\": \"short-term-rentals-registration\" }"
      ],
      "id": "e206c308",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. **Make the API Call:**\n",
        "   Using the `requests` library, we send a GET request to the API to retrieve the data package information:\n"
      ],
      "id": "d53706e4"
    },
    {
      "cell_type": "code",
      "metadata": {
        "md-indent": "   "
      },
      "source": [
        "package = requests.get(url, params=params).json()"
      ],
      "id": "e53b5a4d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3. **Process the Data:**\n",
        "   The API response contains various resources related to the dataset. We loop through these resources, checking if they are active data stores (`datastore_active`). For each active resource, we fetch the data in CSV format:\n"
      ],
      "id": "d100db99"
    },
    {
      "cell_type": "code",
      "metadata": {
        "md-indent": "   "
      },
      "source": [
        "df_big = pd.DataFrame()\n",
        "\n",
        "for idx, resource in enumerate(package[\"result\"][\"resources\"]):\n",
        "    if resource[\"datastore_active\"]:\n",
        "        url = base_url + \"/datastore/dump/\" + resource[\"id\"]\n",
        "        df_temp = pd.read_csv(url)\n",
        "        df_temp['filename'] = url\n",
        "        df_temp['first_seen'] = datetime.now().strftime('%Y-%m-%d')\n",
        "        df_temp['last_active'] = datetime.now().strftime('%Y-%m-%d')\n",
        "        df_big = pd.concat([df_big, df_temp])"
      ],
      "id": "2a4b91de",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "   Here’s what each step does:\n",
        "   - **Fetch CSV Data:** We construct the URL to access the data dump and read it into a pandas DataFrame.\n",
        "   - **Add Metadata:** We include additional columns to keep track of the source URL, the date the data was first seen, and the date it was last active.\n",
        "   - **Concatenate DataFrames:** We combine all the data into a single DataFrame for easier handling.\n",
        "\n",
        "4. **Return the Data:**\n",
        "   Finally, the function returns the combined DataFrame containing all the rental data:\n"
      ],
      "id": "d4d9cffe"
    },
    {
      "cell_type": "code",
      "metadata": {
        "md-indent": "   "
      },
      "source": [
        "return df_big"
      ],
      "id": "02893cf9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Conclusion\n",
        "\n",
        "This {python} script provides a streamlined approach to fetching and processing short-term rental data from an open data API. By automating data retrieval and incorporating metadata, we can efficiently gather and manage large datasets. This method is not only applicable to rental data but can be adapted to various other data sources and types. \n",
        "\n",
        "In summary, using {python} for data collection helps organizations stay up-to-date with the latest information while saving time and resources. Whether you are analyzing rental trends or integrating data into a larger system, this approach lays a solid foundation for effective data management.\n",
        "\n",
        "#### Database Setup and Initialization\n",
        "\n",
        "1. **SQLite Database Creation:**\n",
        "   We create an SQLite database (`ShortTermRental.db`) and define a table `short_term_rentals` to store rental data. The table includes columns for a unique hash, rental details, and date information.\n",
        "\n",
        "2. **Populating Initial Data:**\n",
        "   Using a CSV file with initial rental data, we calculate the unique hash for each record and insert the data into the `short_term_rentals` table. This sets up our base dataset.\n",
        "\n",
        "#### Handling New Data\n",
        "\n",
        "1. **Creating Temporary Table:**\n",
        "   A temporary table (`temp_rental`) is created to hold new rental data fetched periodically. This table mirrors the structure of the main table but is used to manage and compare new entries.\n",
        "\n",
        "2. **Fetching and Inserting New Data:**\n",
        "   New rental data is fetched using the custom script and inserted into the `temp_rental` table. Hash values are generated for this new data to facilitate efficient comparison with existing records.\n",
        "\n",
        "3. **Identifying New Records:**\n",
        "   A query is executed to identify records in `temp_rental` that are not present in `short_term_rentals`. This helps in pinpointing new listings that need to be added to the main database.\n",
        "\n",
        "4. **Updating the Main Database:**\n",
        "   New records are appended to the `short_term_rentals` table. Additionally, the `last_active` field is updated to reflect the most recent activity date for active listings.\n",
        "\n",
        "#### Finalizing Changes\n",
        "\n",
        "1. **Commit and Save:**\n",
        "   Changes are committed to the database to ensure that all updates are saved. The connection to the database is closed to finalize the session.\n",
        "\n",
        "2. **Export New Data:**\n",
        "   A CSV file (`new_information_only.csv`) is generated to keep a record of newly added entries for further analysis or reporting.\n",
        "\n",
        "#### Testing and Validation\n",
        "\n",
        "1. **Verification:**\n",
        "   Various queries are used to verify that the updates and new entries are correctly reflected in the database. This includes checking for records with updated `last_active` dates and ensuring that the data in `short_term_rentals` matches expectations.\n",
        "\n",
        "By following these steps, we ensure that our rental data remains current and accurate. This approach can be adapted to manage other types of datasets and databases, demonstrating a powerful method for data integration and management using {python} and SQLite."
      ],
      "id": "31724ad7"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "C:\\Users\\hbdil\\anaconda3\\envs\\geo_env\\share\\jupyter\\kernels\\python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}