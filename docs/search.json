[
  {
    "objectID": "projects.html#project-2",
    "href": "projects.html#project-2",
    "title": "Projects & Blog",
    "section": "Project 2",
    "text": "Project 2"
  },
  {
    "objectID": "projects.html#project-3",
    "href": "projects.html#project-3",
    "title": "Projects & Blog",
    "section": "Project 3",
    "text": "Project 3"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "A bit about me:",
    "section": "",
    "text": "Currently studying Management Analytics in Toronto. My experience is in retail analytics + viz, demand prediction and lately around applying machine learning to novel business problems.\n\n\n\nCurrent: Masters of Management Analytics at Rotman School of Management\nBachlors of Business Administration at University of Toronto\nUndergraduate Exchange at University of Sydney\n\n\n\n\n\nData Communicator, Freelance\nData Scientist, Indigo Park Canada\nData Scientist, The BRIDGE\nAnalytics Engineer, The BRIDGE\nDigital Insights and Data Analyst, Lactalis Canada\nThanks for checking out my web site!"
  },
  {
    "objectID": "index.html#goals",
    "href": "index.html#goals",
    "title": "My Website (WIP)",
    "section": "",
    "text": "Work on Quarto"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "A bit about me:",
    "section": "",
    "text": "Current: Masters of Management Analytics at Rotman School of Management\nBachlors of Business Administration at University of Toronto\nUndergraduate Exchange at University of Sydney"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects & Blog",
    "section": "",
    "text": "Here are some highlights from my past projects:"
  },
  {
    "objectID": "projects.html#project-1",
    "href": "projects.html#project-1",
    "title": "Projects",
    "section": "",
    "text": "This is project 1"
  },
  {
    "objectID": "newfile.html#backgroundrationale",
    "href": "newfile.html#backgroundrationale",
    "title": "newfiletitle",
    "section": "2.1 Background/rationale",
    "text": "2.1 Background/rationale\n\nExplain the scientific background and rationale for the investigation being reported"
  },
  {
    "objectID": "newfile.html#objectives",
    "href": "newfile.html#objectives",
    "title": "newfiletitle",
    "section": "2.2 Objectives",
    "text": "2.2 Objectives\n\nState specific objectives, including any prespecified hypotheses"
  },
  {
    "objectID": "newfile.html#literature-review",
    "href": "newfile.html#literature-review",
    "title": "newfiletitle",
    "section": "2.3 Literature Review:",
    "text": "2.3 Literature Review:\n\nReview of relevant prior research and theories.\nIdentification of gaps in existing knowledge."
  },
  {
    "objectID": "newfile.html#study-design",
    "href": "newfile.html#study-design",
    "title": "newfiletitle",
    "section": "3.1 Study design",
    "text": "3.1 Study design\n\nPresent key elements of study design early in the paper"
  },
  {
    "objectID": "newfile.html#setting",
    "href": "newfile.html#setting",
    "title": "newfiletitle",
    "section": "3.2 Setting",
    "text": "3.2 Setting\n\nDescribe the setting, locations, and relevant dates, including periods of recruitment, exposure, follow-up, and data collection"
  },
  {
    "objectID": "newfile.html#participants",
    "href": "newfile.html#participants",
    "title": "newfiletitle",
    "section": "3.3 Participants",
    "text": "3.3 Participants\n\nCohort study — Give the eligibility criteria, and the sources and methods of selection of participants. Describe methods of follow-up\nCase-control study — Give the eligibility criteria, and the sources and methods of case ascertainment and control selection. Give the rationale for the choice of cases and controls\nCross-sectional study — Give the eligibility criteria, and the sources and methods of selection of participants\nCohort study — For matched studies, give matching criteria and number of exposed and unexposed\nCase-control study — For matched studies, give matching criteria and the number of controls per case"
  },
  {
    "objectID": "newfile.html#variables",
    "href": "newfile.html#variables",
    "title": "newfiletitle",
    "section": "3.4 Variables",
    "text": "3.4 Variables\n\nClearly define all outcomes, exposures, predictors, potential confounders, and effect modifiers. Give diagnostic criteria, if applicable"
  },
  {
    "objectID": "newfile.html#data-sourcesmeasurement",
    "href": "newfile.html#data-sourcesmeasurement",
    "title": "newfiletitle",
    "section": "3.5 Data sources/measurement",
    "text": "3.5 Data sources/measurement\n\nGive information separately for cases and controls in case-control studies and, if applicable, for exposed and unexposed groups in cohort and cross-sectional studies.\nFor each variable of interest, give sources of data and details of methods of assessment (measurement). Describe comparability of assessment methods if there is more than one group"
  },
  {
    "objectID": "newfile.html#bias",
    "href": "newfile.html#bias",
    "title": "newfiletitle",
    "section": "3.6 Bias",
    "text": "3.6 Bias\n\nDescribe any efforts to address potential sources of bias"
  },
  {
    "objectID": "newfile.html#study-size",
    "href": "newfile.html#study-size",
    "title": "newfiletitle",
    "section": "3.7 Study size",
    "text": "3.7 Study size\n\nExplain how the study size was arrived at"
  },
  {
    "objectID": "newfile.html#quantitative-variables",
    "href": "newfile.html#quantitative-variables",
    "title": "newfiletitle",
    "section": "3.8 Quantitative variables",
    "text": "3.8 Quantitative variables\n\nExplain how quantitative variables were handled in the analyses. If applicable, describe which groupings were chosen and why"
  },
  {
    "objectID": "newfile.html#statistical-methods",
    "href": "newfile.html#statistical-methods",
    "title": "newfiletitle",
    "section": "3.9 Statistical methods",
    "text": "3.9 Statistical methods\n\nDescribe all statistical methods, including those used to control for confounding\nDescribe any methods used to examine subgroups and interactions\nExplain how missing data were addressed\nCohort study — If applicable, explain how loss to follow-up was addressed\nCase-control study — If applicable, explain how matching of cases and controls was addressed\nCross-sectional study — If applicable, describe analytical methods taking account of sampling strategy\nDescribe any sensitivity analyses"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "A bit about me:",
    "section": "",
    "text": "Data Communicator, Freelance\nData Scientist, Indigo Park Canada\nData Scientist, The BRIDGE\nAnalytics Engineer, The BRIDGE\nDigital Insights and Data Analyst, Lactalis Canada"
  },
  {
    "objectID": "newproject.html",
    "href": "newproject.html",
    "title": "Latest",
    "section": "",
    "text": "Task orchestration through tools like crontab or Windows Task Scheduler is useful for automating routine processes and managing system tasks.\nThese tools enable users to schedule scripts and other workloads to run automatically at specified times (such as automatic program updates). By automating these tasks, users can ensure critical operations are performed consistently without manual intervention, reducing the risk of human error and freeing up valuable time in both personal and professional environments.\nIn this series, we will use crontab to autorun some scraping scripts\n\n\n\nAutomation Icon"
  },
  {
    "objectID": "newproject.html#project-1",
    "href": "newproject.html#project-1",
    "title": "Projects",
    "section": "",
    "text": "This is project 1"
  },
  {
    "objectID": "newproject.html#project-2",
    "href": "newproject.html#project-2",
    "title": "Projects",
    "section": "Project 2",
    "text": "Project 2"
  },
  {
    "objectID": "newproject.html#project-3",
    "href": "newproject.html#project-3",
    "title": "Projects",
    "section": "Project 3",
    "text": "Project 3"
  },
  {
    "objectID": "newproject.html#chilmark",
    "href": "newproject.html#chilmark",
    "title": "Detailed Project Sample",
    "section": "",
    "text": "Here is a simple image with a description. This also overrides the description position and places it to the left of the image.\n\n\n\nBeach in Chilmark"
  },
  {
    "objectID": "newproject.html#elsewhere",
    "href": "newproject.html#elsewhere",
    "title": "Detailed Project Sample",
    "section": "Elsewhere",
    "text": "Elsewhere\nThe below demonstrates placing more than one image in a gallery. Note the usage of the layout-ncol which arranges the images on the page side by date. Adding the group attribute to the markdown images places the images in a gallery grouped together based upon the group name provided.\n\n\n\n\n\n\n\n\n\nAquinnah\n\n\n\n\n\n\n\nOak Bluffs\n\n\n\n\n\n\n\n\n\nVineyard lighthouse"
  },
  {
    "objectID": "newproject.html#with-computation-code-chunks",
    "href": "newproject.html#with-computation-code-chunks",
    "title": "Latest",
    "section": "With computation code chunks",
    "text": "With computation code chunks\nOptions for lightbox can be passed using chunk options.\n\nplot(1:10, rnorm(10))\n\n\n\n\nSimple demo R plot\n\n\n\n\n\nplot(cars)\n\n\n\n\nPlot about cars data\n\n\n\n\nIt is possible to create several plots, and group them in a lightbox gallery. Use list in YAML for options when you have several plots, on per plot.\n\nplot(mtcars)\n\n\n\n\nCaption for first plot\n\n\n\nplot(cars)\n\n\n\n\nCaption for second plot\n\n\n\n\nWhen lightbox: auto in main YAML config, you can opt-out lightbox on a plot by setting lightbox: false\n\nplot(mtcars)\n\n\n\n\nmtcars"
  },
  {
    "objectID": "newproject.html#credits",
    "href": "newproject.html#credits",
    "title": "Latest",
    "section": "Credits",
    "text": "Credits\nThe images in this example were used under the Unsplash license, view originals below:\n\nChilmark Beach\nAquinnah\nGingerbread House\nEdgartown Light\nEdgartown Sailboat\n\n\n\n\n\nAutomation. credit: Flaticon Pack\n\n\nThe waves break off the coast of Aquinnah on a beautiful summer day.\n\n\nOak Bluffs is famous for its Gingerbread cottages, busy town center, and party like atmosphere.\n\n\nThe Edgartown Lighthouse is a short walk from downtown and has beautiful views over the entrance to Edgartown Harbor.\n\n\nThis is 1 to 10 plot\n\n\nWe see our cars data above\n\n\nThis is the decription for first graph\n\n\nThis is the decription for second graph"
  },
  {
    "objectID": "newproject.html#task-orchestration",
    "href": "newproject.html#task-orchestration",
    "title": "Latest",
    "section": "",
    "text": "Task orchestration through tools like crontab or Windows Task Scheduler is useful for automating routine processes and managing system tasks.\nThese tools enable users to schedule scripts and other workloads to run automatically at specified times (such as automatic program updates). By automating these tasks, users can ensure critical operations are performed consistently without manual intervention, reducing the risk of human error and freeing up valuable time in both personal and professional environments.\nIn this series, we will use crontab to autorun some scraping scripts\n\n\n\nAutomation Icon"
  },
  {
    "objectID": "newproject.html#section",
    "href": "newproject.html#section",
    "title": "Latest",
    "section": "",
    "text": "The below demonstrates placing more than one image in a gallery. Note the usage of the layout-ncol which arranges the images on the page side by date. Adding the group attribute to the markdown images places the images in a gallery grouped together based upon the group name provided.\n\n\n\n\n\n\n\n\n\nAquinnah\n\n\n\n\n\n\n\nOak Bluffs\n\n\n\n\n\n\n\n\n\nVineyard lighthouse"
  },
  {
    "objectID": "newproject.html#for-linux",
    "href": "newproject.html#for-linux",
    "title": "Latest",
    "section": "For Linux",
    "text": "For Linux\nThe below demonstrates placing more than one image in a gallery. Note the usage of the layout-ncol which arranges the images on the page side by date. Adding the group attribute to the markdown images places the images in a gallery grouped together based upon the group name provided."
  },
  {
    "objectID": "newproject.html#for-windows",
    "href": "newproject.html#for-windows",
    "title": "Latest",
    "section": "For Windows",
    "text": "For Windows\n\n\n\n\n\n\n\n\n\nAquinnah\n\n\n\n\n\n\n\nOak Bluffs\n\n\n\n\n\n\n\n\n\nVineyard lighthouse"
  },
  {
    "objectID": "ST_Rental.html",
    "href": "ST_Rental.html",
    "title": "Short Term Rental",
    "section": "",
    "text": "Tracking Airbnb in Toronto using Registration Data\nThe City of Toronto maintains several open datasets related to city operations - updated at a range of frequencies. This project is focused on the Short Term Rental Registration data (updated daily). This data is useful in tracking active Airbnb and similar short term accomodations in Toronto. In order to retrieve insights from these applications, we’d need to gather the data for analysis. However, the data has some limitations - mainly that only active listings are available, not all the historical data.\nPer the given metadata, A short-term rental is all or part of a dwelling unit rented out for less than 28 consecutive days in exchange for payment. Short-term rentals include bed and breakfasts, but exclude hotels, motels, student residences owned or operated by publicly funded or non-profit educational institutions. See the City’s page for more information.\nIn this article, we walk through a practical example of managing and updating a database of short-term rental listings using and SQLite. The goal is to maintain an up-to-date record of rental properties by regularly incorporating new data and updating existing records. Here’s a step-by-step guide on how this can be achieved.\n\nSetting Up the Environment\nPackage Imports: To begin, we import necessary packages including:\n\npandas for data manipulation,\nsqlite3 for database interactions,\nhashlib for hashing, and\na custom script (ShortTermRental) to fetch from Toronto’s Open Data.\n\nFetching from Toronto’s Open Data\n\n\nSetting Up the Environment\nTo get started, we need to set up our environment by importing the necessary packages. In this case, we’ll use requests to make API calls, pandas for data manipulation, and datetime to handle date and time information.\nimport requests\nimport pandas as pd\nfrom datetime import datetime, date\n\n\nFetching Rental Data\nThe core of our data process involves an API to retrieve short-term rental data:\n\nDefine the API Endpoint: We first define the base URL for the API provided by the City of Toronto’s open data portal. Much of this will be provided by the developper tab on the Open Data Portal. Feel free to do a high level request and navigate around the response to find what you may be looking for.\nMake the API Call: We send a GET request using the requests library to the API to retrieve the data package information\nProcess the Data: The API response contains various resources related to the dataset. We loop through these resources, checking if they are active data stores (datastore_active). For each active resource, we fetch the data in CSV format:\nReturn the Data: Finally, the function returns the combined DataFrame containing all the rental data.\n\ndef fetch_rental_data():\n    # To hit the API, you'll be making requests to:\n    base_url = \"https://ckan0.cf.opendata.inter.prod-toronto.ca\"\n        \n    # Datasets are called \"packages\". Each package can contain many \"resources\"\n    url = base_url + \"/api/3/action/package_show\"\n    params = { \"id\": \"short-term-rentals-registration\"}\n        \n    package = requests.get(url, params = params).json()\n\n   # Placeholder df to hold the extracted information\n    df_big = pd.DataFrame()\n\n    # To get resource data:\n    for idx, resource in enumerate(package[\"result\"][\"resources\"]):\n\n        # for datastore_active resources - returns a bool:\n        if resource[\"datastore_active\"]:\n            # To get all records in CSV format:\n            url = base_url + \"/datastore/dump/\" + resource[\"id\"]\n            \n            df_temp= pd.read_csv(url)\n            df_temp['filename'] = url\n            df_temp['first_seen'] = datetime.now().strftime('%Y-%m-%d')\n            df_temp['last_active'] = datetime.now().strftime('%Y-%m-%d')\n            df_big = pd.concat([df_big,df_temp])\n            \n    return df_big\nThis script provides a streamlined approach to fetching and processing short-term rental data. This method is not only applicable to rental data but can be adapted to various other data sources and types. Feel free to browse Toronto’s Open Data Portal\n\n\nDatabase Setup and Initialization\n\nSQLite Database Creation: We create an SQLite database (ShortTermRental.db) and define a table short_term_rentals to store rental data. The table includes columns for a unique hash, rental details, and date information.\nPopulating Initial Data: Using a CSV file with initial rental data, we calculate the unique hash for each record and insert the data into the short_term_rentals table. This sets up our base dataset.\n\n\n\nHandling New Data\n\nHashing Function: A hash function is used to create a unique identifier for each rental record based on specific columns such as operator_registration_number, address, unit, postal_code, property_type, ward_number, and ward_name. This identifier helps in quickly comparing new records with existing ones to identify updates or new entries.\nCreating Temporary Table: A temporary table (temp_rental) is created to hold new rental data fetched periodically. This table mirrors the structure of the main table but is used to manage and compare new entries.\nFetching and Inserting New Data: New rental data is fetched using the custom script and inserted into the temp_rental table. Hash values are generated for this new data to facilitate efficient comparison with existing records.\nIdentifying New Records: A query is executed to identify records in temp_rental that are not present in short_term_rentals. This helps in pinpointing new listings that need to be added to the main database.\nUpdating the Main Database: New records are appended to the short_term_rentals table. Additionally, the last_active field is updated to reflect the most recent activity date for active listings.\n\n\n\nFinalizing Changes\n\nCommit and Save: Changes are committed to the database to ensure that all updates are saved. The connection to the database is closed to finalize the session.\nExport New Data: A CSV file (new_information_only.csv) is generated to keep a record of newly added entries for further analysis or reporting.\n\n\n\nTesting and Validation\n\nVerification: Various queries are used to verify that the updates and new entries are correctly reflected in the database. This includes checking for records with updated last_active dates and ensuring that the data in short_term_rentals matches expectations.\n\nBy following these steps, we ensure that our rental data remains current and accurate. This approach can be adapted to manage other types of datasets and databases, demonstrating a powerful method for data integration and management using and SQLite.\n\n\n\nVisualization\nOnce we are able to reliably collect historical information, we can start to visualize the information through Power BI for example"
  },
  {
    "objectID": "ST_Rental.html#task-orchestration",
    "href": "ST_Rental.html#task-orchestration",
    "title": "Latest",
    "section": "",
    "text": "In this article, we walk through a practical example of managing and updating a database of short-term rental listings using Python and SQLite. The goal is to maintain an up-to-date record of rental properties by regularly incorporating new data and updating existing records. Here’s a step-by-step guide on how this can be achieved.\n\n\n\nPackage Imports: To begin, we import necessary Python packages including pandas for data manipulation, sqlite3 for database interactions, hashlib for hashing, and a custom script (ShortTermRental) to fetch new rental data.\nHashing Function: A hash function is used to create a unique identifier for each rental record based on specific columns such as operator_registration_number, address, unit, postal_code, property_type, ward_number, and ward_name. This identifier helps in quickly comparing new records with existing ones to identify updates or new entries.\n\n\n\n\n\nSQLite Database Creation: We create an SQLite database (ShortTermRental.db) and define a table short_term_rentals to store rental data. The table includes columns for a unique hash, rental details, and date information.\nPopulating Initial Data: Using a CSV file with initial rental data, we calculate the unique hash for each record and insert the data into the short_term_rentals table. This sets up our base dataset.\n\n\n\n\n\nCreating Temporary Table: A temporary table (temp_rental) is created to hold new rental data fetched periodically. This table mirrors the structure of the main table but is used to manage and compare new entries.\nFetching and Inserting New Data: New rental data is fetched using the custom script and inserted into the temp_rental table. Hash values are generated for this new data to facilitate efficient comparison with existing records.\nIdentifying New Records: A query is executed to identify records in temp_rental that are not present in short_term_rentals. This helps in pinpointing new listings that need to be added to the main database.\nUpdating the Main Database: New records are appended to the short_term_rentals table. Additionally, the last_active field is updated to reflect the most recent activity date for active listings.\n\n\n\n\n\nCommit and Save: Changes are committed to the database to ensure that all updates are saved. The connection to the database is closed to finalize the session.\nExport New Data: A CSV file (new_information_only.csv) is generated to keep a record of newly added entries for further analysis or reporting.\n\n\n\n\n\nVerification: Various queries are used to verify that the updates and new entries are correctly reflected in the database. This includes checking for records with updated last_active dates and ensuring that the data in short_term_rentals matches expectations.\n\nBy following these steps, we ensure that our rental data remains current and accurate. This approach can be adapted to manage other types of datasets and databases, demonstrating a powerful method for data integration and management using Python and SQLite."
  },
  {
    "objectID": "projects.html#tracking-short-term-rental-in-toronto",
    "href": "projects.html#tracking-short-term-rental-in-toronto",
    "title": "Projects & Blog",
    "section": "Tracking Short Term Rental in Toronto",
    "text": "Tracking Short Term Rental in Toronto\nThis project fetches open data to store in a SQLite Database. Since only active registrations are listed, a database tracks status across time. A unique hash is generated for each address, and compares the differentials to update each location accordingly.\nRequests Hashlib SQLite3"
  },
  {
    "objectID": "projects.html#basic-task-orchestration",
    "href": "projects.html#basic-task-orchestration",
    "title": "Projects & Blog",
    "section": "Basic Task Orchestration",
    "text": "Basic Task Orchestration\nThis blog walks through script automation using Task Scheduler and Crontab. These tools can programatically run py files, and others - eliminating manual interactions. Naturally, these work as long as the machine is powered, and thus are best suited for remote VMs running 24/7 Crontab Windows Task Scheduler Batch Files"
  },
  {
    "objectID": "projects.html#webscraping-and-accessing-values-from-console",
    "href": "projects.html#webscraping-and-accessing-values-from-console",
    "title": "Projects & Blog",
    "section": "Webscraping and accessing values from console",
    "text": "Webscraping and accessing values from console\nSending AJAX requests to backends may be useful in certain projects, but authentication/bearer/JWT tokens may limit the success of such requests. Thus, this project uses Selenium to simulate a user and extract console values, such as cookies, or bearer tokens.\nSelenium Requests"
  },
  {
    "objectID": "projects.html#stochastic-oscillator",
    "href": "projects.html#stochastic-oscillator",
    "title": "Projects & Blog",
    "section": "Stochastic Oscillator",
    "text": "Stochastic Oscillator\nStochastic Oscilallators are from a family of technical indicators that can be used to infer buy/sell signals. This project pulls TSX data from yfinance, performs the technical analysis and returns a chart of buy and sell signals. As only historical data is used, thus this project serves as a technical showcase, rather than a tool for recommendations\nyfinance Pandas Matplotlib"
  },
  {
    "objectID": "TaskOrchestration.html",
    "href": "TaskOrchestration.html",
    "title": "Task Orchestration",
    "section": "",
    "text": "Task orchestration through tools like Crontab or Windows Task Scheduler is useful for automating routine processes and managing system tasks.\nThese tools enable users to schedule scripts and other workloads to run automatically at specified times (such as automatic program updates). By automating these tasks, users can ensure critical operations are performed consistently without manual intervention, reducing the risk of human error and freeing up valuable time in both personal and professional environments.\nIn this article, we will use crontab to autorun the sample meteostat script - which visualizes weather data.\n\n\n\nAutomation Icon"
  },
  {
    "objectID": "TaskOrchestration.html#task-orchestration",
    "href": "TaskOrchestration.html#task-orchestration",
    "title": "Task Orchestration",
    "section": "",
    "text": "Task orchestration through tools like Crontab or Windows Task Scheduler is useful for automating routine processes and managing system tasks.\nThese tools enable users to schedule scripts and other workloads to run automatically at specified times (such as automatic program updates). By automating these tasks, users can ensure critical operations are performed consistently without manual intervention, reducing the risk of human error and freeing up valuable time in both personal and professional environments.\nIn this article, we will use crontab to autorun the sample meteostat script - which visualizes weather data.\n\n\n\nAutomation Icon"
  },
  {
    "objectID": "TaskOrchestration.html#for-linux",
    "href": "TaskOrchestration.html#for-linux",
    "title": "Task Orchestration",
    "section": "For Linux",
    "text": "For Linux\n\n1. Open Terminal\nCrontab can be accessed via the terminal using crontab -e to open the crontab menu.\n\n\n\nCrontab\n\n\n\n\n2. Edit Crontab\nIf doing this for the first time, there’ll be a prompted to select the menu.\nSelect an editor. To change later, run 'select-editor'.\n 1. /bin/nano &lt;---- easiest\n 2. /usr/bin/vim.basic\n 3. /usr/bin/vim.tiny\n 4. /bin/ed\n\nChoose 1-4 [1]:\nEnter 1 to choose /bin/nano.\nOnce crontab is set to open with nano, we can pass crontab – e again to view the editor. Scroll to the bottom of the comments and you will be able to set a regular interval and the path to your file.\n\n\n3. Add a Cron Job\nThe five asterisks represent time and date fields:\n\n“*” (minute) - 0-59\n“*” (hour) - 0-23\n“*” (day of the month) - 1-31\n“*” (month) - 1-12\n“*” (day of the week) - 0-7 (0 and 7 both represent Sunday)\n\nSo, for example\n5 10 * * 3 python \"/home/Notebook_Projects/WeatherData.py\"\n\n\n\nCrontabWithCommand\n\n\nThe above means that the program at /home/Notebook_Projects/WeatherData.py will be run at 10:05am on every Wednesday (so long as the device is powered on). You can check the correct time using the crontab.guru service. You can find the copy the path from the file explorer.\n\n\n4. Save and Exit\nIf you’re using nano, save your changes by pressing CTRL + X, then Y to confirm, and Enter to save.\n\n\n5. Verify Cron Jobs\nTo view the current list of cron jobs for the user, use crontab -l from the terminal"
  },
  {
    "objectID": "TaskOrchestration.html#for-windows",
    "href": "TaskOrchestration.html#for-windows",
    "title": "Task Orchestration",
    "section": "For Windows",
    "text": "For Windows\nTo autorun a python file from Windows, you will need a batch file to pass the command automatically into a terminal. In this example, we will use the meteostat demo to visualize max, min and avg temperature.\n\n1. Create a Batch File\n\nOpen Notepad to write your Batch Script. Pass the path of your py file to the python command to execute the file. For example, let’s create a batch file that downloads weather data:\n\npython \"C:\\Users\\User1\\Desktop\\Notebook_Projects\\WeatherData.py\"\n\n\n2. Save the Batch File:\nGo to File -&gt; Save As. In the Save as type dropdown, select All Files. Name the file with a .bat extension, for example, AutomatedTask.bat. Save it to a location of your choice. You will need to navigate to it later.\n\n\n\nNotebookBatchFile\n\n\n\n\n3. Schedule the Batch File with Task Scheduler\nOpen Task Scheduler and in the right-hand pane, click on Create Basic Task. Name and describe Your Task\n\n\n\nCreateBasicTask\n\n\nGive your task a name, like “Automatic Python Task”, and add a description if desired.\n\n\n\nCreateBasicTask1\n\n\nSet the Trigger. Choose when you want the task to start. For example, select Daily if you want it to run every day. Click Next, and then set the start date and time.\n \n\n\n4. Choose the Action:\nSelect Start a program and click Next. Specify the batch file by navigating to the location of your batch file (C:_Projects.bat.). \n\n\n5. Finish the Task:\nReview your settings and click Finish to create the task.\nCheck Task Scheduler - your new task will be in the same folder as you had previously stored it.\n\n\n\nTaskScheduled\n\n\nRun the Task Manually (Optional): Right-click the task and select Run to test if it executes as expected.\n\n\n\nTaskRunSuccessfully\n\n\n\n\nAdditional Tips\nRunning as Administrator: If your script requires administrative privileges, you may need to configure the task to run with the highest privileges. This can be done in the task’s properties under the General tab by checking Run with highest privileges.\n\n\n\n\nAutomation. credit: Flaticon Pack"
  },
  {
    "objectID": "TaskOrchestration.html#with-computation-code-chunks",
    "href": "TaskOrchestration.html#with-computation-code-chunks",
    "title": "Task Orchestration",
    "section": "With computation code chunks",
    "text": "With computation code chunks\nOptions for lightbox can be passed using chunk options.\n\nplot(1:10, rnorm(10))\n\n\n\n\nSimple demo R plot\n\n\n\n\n\nplot(cars)\n\n\n\n\nPlot about cars data\n\n\n\n\nIt is possible to create several plots, and group them in a lightbox gallery. Use list in YAML for options when you have several plots, on per plot.\n\nplot(mtcars)\n\n\n\n\nCaption for first plot\n\n\n\nplot(cars)\n\n\n\n\nCaption for second plot\n\n\n\n\nWhen lightbox: auto in main YAML config, you can opt-out lightbox on a plot by setting lightbox: false\n\nplot(mtcars)\n\n\n\n\nmtcars"
  },
  {
    "objectID": "TaskOrchestration.html#credits",
    "href": "TaskOrchestration.html#credits",
    "title": "Task Orchestration",
    "section": "Credits",
    "text": "Credits\nThe images in this example were used under the Unsplash license, view originals below:\n\nChilmark Beach\nAquinnah\nGingerbread House\nEdgartown Light\nEdgartown Sailboat\n\n\n\n\n\nAutomation. credit: Flaticon Pack\n\n\nThe waves break off the coast of Aquinnah on a beautiful summer day.\n\n\nOak Bluffs is famous for its Gingerbread cottages, busy town center, and party like atmosphere.\n\n\nThe Edgartown Lighthouse is a short walk from downtown and has beautiful views over the entrance to Edgartown Harbor.\n\n\nThis is 1 to 10 plot\n\n\nWe see our cars data above\n\n\nThis is the decription for first graph\n\n\nThis is the decription for second graph"
  },
  {
    "objectID": "projects.html#predicting-e-coli-levels-in-toronto-beaches",
    "href": "projects.html#predicting-e-coli-levels-in-toronto-beaches",
    "title": "Projects & Blog",
    "section": "Predicting E-Coli Levels in Toronto Beaches",
    "text": "Predicting E-Coli Levels in Toronto Beaches\nHigh E-Coli levels in Toronto’s beaches signals that the water is unsafe, and is closed off to the public. This disrupts water activities including local recreation businesses. This project seeks to train a predictive model to identify future high e-coli levels.\nRequests SKLearn Pandas"
  },
  {
    "objectID": "projects.html#simple-task-orchestration",
    "href": "projects.html#simple-task-orchestration",
    "title": "Projects & Blog",
    "section": "Simple Task Orchestration",
    "text": "Simple Task Orchestration\nThis blog walks through script automation using Task Scheduler and Crontab. These tools can programatically run py files, and others - eliminating manual interactions. Naturally, these work as long as the machine is powered, and thus are best suited for remote VMs running 24/7\nCrontab Windows Task Scheduler Batch Files"
  },
  {
    "objectID": "ConsoleValues.html#using-saved-values",
    "href": "ConsoleValues.html#using-saved-values",
    "title": "Console Values",
    "section": "Using saved values",
    "text": "Using saved values\nFor example, this retailer serves area specific search results - and the closest location is auto determined and stored in the browser’s cookies. It also occasionally runs other tests and can categorize the user into segments – further personalizing the search results to a profile.\nTo programatically access similar search results, we want to recreate the same cookies when sending only a request.\nWe observe that the search request reponds with these structured results:\n\n\n\nSearchResults\n\n\nby using the following (select) cookies:\ncookies = {\n    'userSegment': '40-percent',\n    'wm_route_based_language': 'en-CA',\n    'WM_SEC.AUTH_TOKEN': '██████████████████████████████████████████████████████████████████████████████',\n    'userAppVersion': 'main-1.108.2-04dc79b-0917T0153',\n    '██████.nearestPostalCode': '██████',\n    '██████.nearestLatLng': '\"███████,-███████\"',\n    'WM.USER_STATE': 'GUEST%7CGuest',\n    'enableHTTPS': '1',\n    'adblocked': 'true'\n}\nA full list can be seen by inspecting the request:\n\n\n\nCookiesNetwork\n\n\nThe console tab can return these values using document.cookie:\n\n\n\nCookieExtract\n\n\nIn practice, we can arrive to a similar set of web results by navigating to the retailer and extracting the storage values:\n\n\n\nConsoleExtract\n\n\nAfter formatting the cookies from the console into a dict, it is ready to be used in a request fetching personalized results:\n\n\n\nCustomizedSearchWithRequest"
  },
  {
    "objectID": "ConsoleValues.html",
    "href": "ConsoleValues.html",
    "title": "Console Values",
    "section": "",
    "text": "When a page is initially shown, it may just be static content from the server. Objects like cookies and auth tokens can allow you to acccess more personalized results during browsing. This is common when a site serves search results (specific for a given local area) or for Progressive Web Apps. Being able to access the session or local storage allows for programmatic access to those personalized results when sending requests.\n\n\nIn those new requests, tokens are often used. Certain requests require authentication and authorization to ensure permissions and similar information. JSON Web Tokens (JWT) and cookies are often employed for this purpose:\n\nJSON Web Tokens (JWT): JWTs are like digital ID cards with an identity and associated permissions. When you log in, the server gives you this token, which you send back with your requests. The server checks the token to confirm you’re allowed to access certain parts of the site. These auth tokens can be used to authenticate and perform actions as if you were logged in. Naturally, this can be misused and caution is urged.\nCookies: These are small pieces of data stored in the user’s browser, which can also carry authentication information. Cookies can be set to automatically accompany requests to the server when using a browser, but need to be extracted when relying on an explicit request."
  },
  {
    "objectID": "ConsoleValues.html#why-access-values-from-storage",
    "href": "ConsoleValues.html#why-access-values-from-storage",
    "title": "Console Values",
    "section": "",
    "text": "When a page is initially shown, it may just be static content from the server. Objects like cookies and auth tokens can allow you to acccess more personalized results during browsing. This is common when a site serves search results (specific for a given local area) or for Progressive Web Apps. Being able to access the session or local storage allows for programmatic access to those personalized results when sending requests.\n\n\nIn those new requests, tokens are often used. Certain requests require authentication and authorization to ensure permissions and similar information. JSON Web Tokens (JWT) and cookies are often employed for this purpose:\n\nJSON Web Tokens (JWT): JWTs are like digital ID cards with an identity and associated permissions. When you log in, the server gives you this token, which you send back with your requests. The server checks the token to confirm you’re allowed to access certain parts of the site. These auth tokens can be used to authenticate and perform actions as if you were logged in. Naturally, this can be misused and caution is urged.\nCookies: These are small pieces of data stored in the user’s browser, which can also carry authentication information. Cookies can be set to automatically accompany requests to the server when using a browser, but need to be extracted when relying on an explicit request."
  },
  {
    "objectID": "ConsoleValues.html#simulating-browser-visits-with-selenium",
    "href": "ConsoleValues.html#simulating-browser-visits-with-selenium",
    "title": "Console Values",
    "section": "Simulating Browser Visits with Selenium",
    "text": "Simulating Browser Visits with Selenium\nWhen using tools like Selenium, we can simulate actual browser visits and actions, allowing us to perform tasks like logging in to a site, or inputing location data, which generates the necessary cookies and tokens in the browser’s storage. The console allows us to use these cookies and tokens just like a real user would. We may want to extract those and directly send requests with updated and relevant cookies."
  }
]